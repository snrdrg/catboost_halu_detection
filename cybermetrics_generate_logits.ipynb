{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402358e-0645-4127-8244-bf4a7ff538e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26448,
     "status": "ok",
     "timestamp": 1737104926827,
     "user": {
      "displayName": "Anton Mitrofanov",
      "userId": "06016137307846439341"
     },
     "user_tz": -180
    },
    "id": "0402358e-0645-4127-8244-bf4a7ff538e3",
    "outputId": "05f4b94b-9c65-4fa6-a3d9-ae68a6e65aa9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import collections\n",
    "import sys\n",
    "if sys.version_info.major == 3 and sys.version_info.minor >= 10:\n",
    "    from collections.abc import MutableSet\n",
    "    collections.MutableSet = collections.abc.MutableSet\n",
    "    from collections.abc import MutableMapping\n",
    "    collections.MutableMapping = collections.abc.MutableMapping\n",
    "else:\n",
    "    from collections import MutableSet\n",
    "    from collections import MutableMapping\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, pipeline, BitsAndBytesConfig\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import requests as rq\n",
    "import gc\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60685d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(x):\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except Exception:\n",
    "        return {'answer': '', 'explanation': ''}\n",
    "\n",
    "def clear_answer(x):\n",
    "    trns = str.maketrans({'[': None, ']': None, '.': None,})\n",
    "    return x.translate(trns)\n",
    "\n",
    "def parse_output(s):\n",
    "    try:\n",
    "        return json.loads(s[s.find('{'):s.find('}') + 1])\n",
    "    except Exception:\n",
    "        return {'answer': '', 'explanation': ''}\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/snrdrg/kgl/refs/heads/main/CyberMetric-10000-v1.json\"\n",
    "dataset = rq.get(url)\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hf_token = \"\" #here should be HF token with access to LLAMA family models\n",
    "login(hf_token)\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uLvl9HLWoVPv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8989,
     "status": "ok",
     "timestamp": 1737104940072,
     "user": {
      "displayName": "Anton Mitrofanov",
      "userId": "06016137307846439341"
     },
     "user_tz": -180
    },
    "id": "uLvl9HLWoVPv",
    "outputId": "64a8c0e7-257b-4748-a4d4-f18d5b405ce9"
   },
   "outputs": [],
   "source": [
    "class StoppingCriteriaEndWith(StoppingCriteria):\n",
    "    def __init__(self, stop_token, tokenizer):\n",
    "        self.stop_token = tokenizer.encode(stop_token, add_special_tokens=False)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0, -len(self.stop_token):].tolist() == self.stop_token:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "data = json.loads(dataset.content)['questions']\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"./offload\",\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "rag_line = '<system>: Using <end> tag is STRICT rule. STRICT rule dont say anything after <end> tag. Give answer to question in following format: {\"answer\":\"<Only letter of answer>\", \"explanation\": \"<Explanation>\"} <end>'\n",
    "input_text = rag_line + '\\n<user>:' + data[i]['question']\n",
    "input_text = input_text + 'Possible answers:'\n",
    "\n",
    "for k,v in data[i]['answers'].items():\n",
    "    input_text = input_text + k + ':' + v + ';'\n",
    "\n",
    "input_text = input_text + '\\n<assistant>: '\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "\n",
    "inputs_attention_mask = input_ids != tokenizer.pad_token_id\n",
    "stop_token = \" <end>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db85849",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=512,\n",
    "        num_return_sequences=1,\n",
    "        output_logits=True,\n",
    "        return_dict_in_generate=True,\n",
    "        attention_mask=inputs_attention_mask,\n",
    "        stopping_criteria=StoppingCriteriaList([StoppingCriteriaEndWith(stop_token, tokenizer)]),\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_ids = output.sequences\n",
    "logits = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e20cd-80e6-4748-b921-b67f8c44eb81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 1756,
     "status": "error",
     "timestamp": 1737104941826,
     "user": {
      "displayName": "Anton Mitrofanov",
      "userId": "06016137307846439341"
     },
     "user_tz": -180
    },
    "id": "df3e20cd-80e6-4748-b921-b67f8c44eb81",
    "outputId": "e6b6b3cb-652f-4acd-a2a8-ca389e4463f5"
   },
   "outputs": [],
   "source": [
    "class StoppingCriteriaEndWith(StoppingCriteria):\n",
    "    def __init__(self, stop_token, tokenizer):\n",
    "        self.stop_token = tokenizer.encode(stop_token, add_special_tokens=False)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0, -len(self.stop_token):].tolist() == self.stop_token:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "data = json.loads(dataset.content)['questions']\n",
    "df = pd.DataFrame(columns=['question', 'answer', 'right_answer', 'probs', 'medians', 'means', 'stds', 'modes', 'entropy','prev_dist'])\n",
    "n = len(data)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"./offload\",\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "print('device:', model.device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "f_idx = 0\n",
    "\n",
    "for i in tqdm(range(n)):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    rag_line = '<system>: Using <end> tag is STRICT rule. STRICT rule dont say anything after <end> tag. Give answer to question in following format: {\"answer\":\"<Only letter of answer>\", \"explanation\": \"<Explanation>\"} <end>'\n",
    "    input_text = rag_line + '\\n<user>:' + data[i]['question']\n",
    "    input_text = input_text + 'Possible answers:'\n",
    "    for k,v in data[i]['answers'].items():\n",
    "        input_text = input_text + k + ':' + v + ';'\n",
    "    input_text = input_text + '\\n<assistant>: '\n",
    "\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "    inputs_attention_mask = input_ids != tokenizer.pad_token_id\n",
    "    stop_token = \" <end>\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=512,\n",
    "            num_return_sequences=1,\n",
    "            output_logits=True,\n",
    "            return_dict_in_generate=True,\n",
    "            attention_mask=inputs_attention_mask,\n",
    "            stopping_criteria=StoppingCriteriaList([StoppingCriteriaEndWith(stop_token, tokenizer)]),\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_ids = output.sequences\n",
    "    logits = output.logits\n",
    "    log_max = []\n",
    "    log_med = []\n",
    "    log_mean = []\n",
    "    log_std = []\n",
    "    log_mode = []\n",
    "    log_entropy = []\n",
    "    log_similarity = []\n",
    "    prev_logits = None\n",
    "    \n",
    "    for row in logits:\n",
    "        probs = F.softmax(row, dim=-1).cpu().numpy()\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-12))        \n",
    "        if prev_logits is not None:\n",
    "            dist = 1 - cosine(row.cpu().numpy()[0], prev_logits[0])\n",
    "        else:\n",
    "            dist = 0\n",
    "        prev_logits = row.cpu().numpy()\n",
    "        \n",
    "        lgt = row[-1].to('cpu').numpy()\n",
    "        log_max.append(lgt.argmax())\n",
    "        log_med.append(np.median(lgt))\n",
    "        log_mean.append(lgt.mean())\n",
    "        log_std.append(lgt.std())\n",
    "        log_mode.append(stats.mode(lgt)[0])\n",
    "        log_entropy.append(entropy)\n",
    "        log_similarity.append(dist)\n",
    "        \n",
    "        \n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    if generated_text.startswith(input_text):\n",
    "        generated_text = generated_text[len(input_text):].strip()\n",
    "\n",
    "    df.loc[i] = [data[i]['question'], generated_text, data[i]['solution'], log_max, log_med, log_mean, log_std, log_mode, log_entropy, log_similarity]\n",
    "    if df.shape[0] >= 1000:\n",
    "        print('Saving chunk', f_idx)        \n",
    "        df['probs'] = df['probs'].apply(lambda x: str(x))\n",
    "        df['medians'] = df['medians'].apply(lambda x: str(x))\n",
    "        df['means'] = df['means'].apply(lambda x: str(x))\n",
    "        df['stds'] = df['stds'].apply(lambda x: str(x))\n",
    "        df['modes'] = df['modes'].apply(lambda x: str(x))\n",
    "        df['entropy'] = df['entropy'].apply(lambda x: str(x))\n",
    "        df['prev_dist'] = df['prev_dist'].apply(lambda x: str(x))\n",
    "        df.to_csv('llama323b_8bit_cyber' + str(f_idx) + '.csv', index=False)      \n",
    "        del df\n",
    "        gc.collect()\n",
    "        df = pd.DataFrame(columns=['question', 'answer', 'right_answer', 'probs', 'medians', 'means', 'stds', 'modes', 'entropy','prev_dist'])\n",
    "        f_idx += 1\n",
    "        print('Saving complete, continue')\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if df.shape[0] > 0:\n",
    "    print('Saving chunk', f_idx)        \n",
    "    df['probs'] = df['probs'].apply(lambda x: str(x))\n",
    "    df['medians'] = df['medians'].apply(lambda x: str(x))\n",
    "    df['means'] = df['means'].apply(lambda x: str(x))\n",
    "    df['stds'] = df['stds'].apply(lambda x: str(x))\n",
    "    df['modes'] = df['modes'].apply(lambda x: str(x))\n",
    "    df['entropy'] = df['entropy'].apply(lambda x: str(x))\n",
    "    df['prev_dist'] = df['prev_dist'].apply(lambda x: str(x))\n",
    "    df.to_csv('llama323b_8bit_cyber' + str(f_idx) + '.csv', index=False) \n",
    "    df = pd.DataFrame(columns=['question', 'answer', 'right_answer', 'probs'])\n",
    "    print('Saving complete, continue')\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print('Inference finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6894f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.shape[0] > 0:\n",
    "    print('Saving chunk', f_idx)        \n",
    "    df['probs'] = df['probs'].apply(lambda x: str(x))\n",
    "    df['medians'] = df['medians'].apply(lambda x: str(x))\n",
    "    df['means'] = df['means'].apply(lambda x: str(x))\n",
    "    df['stds'] = df['stds'].apply(lambda x: str(x))\n",
    "    df['modes'] = df['modes'].apply(lambda x: str(x))\n",
    "    df['entropy'] = df['entropy'].apply(lambda x: str(x))\n",
    "    df['prev_dist'] = df['prev_dist'].apply(lambda x: str(x))\n",
    "    df.to_csv('llama323b_8bit_cyber' + str(f_idx) + '.csv', index=False) \n",
    "    df = pd.DataFrame(columns=['question', 'answer', 'right_answer', 'probs'])\n",
    "    print('Saving complete, continue')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1qTWmXemg53y_2qcN9Z3h-kjEAcQQsUxN",
     "timestamp": 1737104418340
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
